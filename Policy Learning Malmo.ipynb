{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from Simulation import Level\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# L1 and L2 are simple and complex examples we will use to test policy\n",
    "# learning\n",
    "L1 = [\"11111\",\n",
    "     \"12221\",\n",
    "      \"12221\",\n",
    "      \"12221\",\n",
    "     \"12121\",\n",
    "     \"12121\",\n",
    "     \"11X11\"]\n",
    "\n",
    "L2 = [\"11111\",\n",
    "     \"00001\",\n",
    "     \"00011\",\n",
    "     \"21111\",\n",
    "     \"11111\",\n",
    "     \"12000\",\n",
    "     \"22000\",\n",
    "     \"11011\",\n",
    "     \"12111\",\n",
    "     \"11111\",\n",
    "     \"11X11\"]\n",
    "\n",
    "simple_level = Level(L1)\n",
    "complex_level = Level(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111\n",
      "12221\n",
      "12221\n",
      "12221\n",
      "12121\n",
      "12121\n",
      "11X11\n",
      "\n",
      "11111\n",
      "00001\n",
      "00011\n",
      "21111\n",
      "11111\n",
      "12000\n",
      "22000\n",
      "11011\n",
      "12111\n",
      "11111\n",
      "11X11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(simple_level)\n",
    "print(complex_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "agent_view = 5*5*3\n",
    "agent_choices = 6\n",
    "learning_rate = 0.001\n",
    "gamma = 0.01\n",
    "hidden_size = 128\n",
    "dropout_prob = 0\n",
    "epsilon = 0.1\n",
    "episodeNumber = 0\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = agent_view # Input vector\n",
    "        self.action_space = agent_choices # Number of choices\n",
    "        \n",
    "        # Neural Net architecture\n",
    "        self.l1 = nn.Linear(self.state_space, hidden_size, bias=True)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.l3 = nn.Linear(hidden_size, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Episode policy and reward history\n",
    "        self.policy_history = Variable(torch.Tensor())\n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.SELU(),\n",
    "            self.l2,\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.SELU(),\n",
    "            self.l3,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement select_action here\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "    choices = policy(Variable(state))\n",
    "    c = Categorical(choices)\n",
    "    action = c.sample()\n",
    "    \n",
    "    if(random.random() < epsilon):\n",
    "        tempArray = np.array([0.166,0.166,0.166,0.166,0.166,0.167])\n",
    "        choices2 = torch.Tensor(tempArray)\n",
    "        c2 = Categorical(choices2)\n",
    "        action = c2.sample()\n",
    "    \n",
    "    if policy.policy_history.nelement() == 0:\n",
    "        policy.policy_history = torch.stack([c.log_prob(action)])\n",
    "    else:\n",
    "        policy.policy_history = torch.cat([policy.policy_history, torch.stack([c.log_prob(action)])])\n",
    "\n",
    "    return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply Monte-Carlo Policy Gradient to improve out policy according\n",
    "# to the equation\n",
    "def update_policy():\n",
    "    \n",
    "    R = 0\n",
    "    rewards = []\n",
    "\n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0,R)\n",
    "\n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float64).eps)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, Variable(rewards)).mul(-1), -1))\n",
    "\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    policy.loss_history.append(loss.data.item())\n",
    "    \n",
    "    #Save and intialize episode history counter\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = Variable(torch.Tensor())\n",
    "    policy.reward_episode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfunc0(x,steps,done):\n",
    "    reward = 0\n",
    "    if done:\n",
    "        if(x <= 0):\n",
    "            reward = 100000000000 - (20*steps)\n",
    "        else:\n",
    "            reward = 0\n",
    "    else:\n",
    "        reward = 3000 - (200*x) - (100*steps)\n",
    "    #reward -= steps/4\n",
    "    return reward\n",
    "\n",
    "\n",
    "def rfunc1(x,steps,done):\n",
    "    reward = 20 - (x*3)\n",
    "    if done:\n",
    "        if(x <= 0):\n",
    "            reward += 40 - steps\n",
    "        else:\n",
    "            reward -= 40 + steps\n",
    "    #reward -= steps/4\n",
    "    return reward\n",
    "\n",
    "def rfunc2(x,steps,done):\n",
    "    return 1/(x+2)\n",
    "\n",
    "def rfunc3(x, steps, done):\n",
    "    return random.random()\n",
    "\n",
    "def rfunc4(x, steps, done):\n",
    "    return 5-x\n",
    "\n",
    "level = complex_level\n",
    "max_reward = 1\n",
    "\n",
    "ActDictionary = {-1:\"IN\",\n",
    "                 0:\"SU\",\n",
    "                 1:\"SL\",\n",
    "                 2:\"SR\",\n",
    "                 3:\"JU\",\n",
    "                 4:\"JL\",\n",
    "                 5:\"JR\"}\n",
    "\n",
    "def main(episodes):\n",
    "    global episodeNumber\n",
    "    episodeNumber = 0\n",
    "    global epsilon\n",
    "    truMinX = 200\n",
    "    storeSteps = None\n",
    "    for episode in range(episodes):\n",
    "        minX = 200\n",
    "        episodeNumber += 1\n",
    "        epsilon = 2/(math.log(episodeNumber+2,2)+1)\n",
    "        done = False     \n",
    "        level.Reset()\n",
    "        stps = []\n",
    "        x,steps,done = level.getLivingRewardState()\n",
    "        prevR = rfunc0(x, steps, done)\n",
    "        while not done:\n",
    "            state = np.asarray(level.getVector())\n",
    "            action = select_action(state)\n",
    "            x,steps,done = level.Act(action)\n",
    "            reward = rfunc0(x,steps,done)\n",
    "            stps += [(ActDictionary[action], reward - prevR)]\n",
    "            if(x < minX):\n",
    "                minX = x\n",
    "            policy.reward_episode.append(reward - prevR)\n",
    "            prevR = reward\n",
    "            if x <= 0:\n",
    "                print(\"\\nReached the end!\",end=\" \")\n",
    "                print(episodeNumber)\n",
    "                print(stps)\n",
    "            if done:\n",
    "                break \n",
    "        update_policy()\n",
    "        #print(\"Episode Done!\")\n",
    "        if(minX < truMinX):\n",
    "            storeSteps = stps\n",
    "    print(storeSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached the end! 849\n",
      "[('JU', 300), ('JL', -100), ('SU', 100), ('SU', 100), ('SU', 100), ('SU', 100), ('JR', -100), ('JR', -100), ('SU', 100), ('SU', 100), ('SU', 100), ('SU', 99999998060)]\n",
      "[('JU', 300), ('SR', -100), ('SU', 100), ('SU', -1300)]\n"
     ]
    }
   ],
   "source": [
    "main(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
